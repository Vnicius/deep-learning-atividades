{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\Miniconda3\\envs\\agoravai\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em um problema real de Redes Neurais, normalmente uma instância pode possuir inúmeras entradas, resultando em uma arquitetura de rede extensa com muitos neurônios e de múltiplas camadas. \n",
    "\n",
    "Um exemplo comum de aplicação nesse escopo é um classificador de imagens, em que cada entrada representa um neurônio da camada de entrada. Sabendo disso, esse notebook tem como objetivo implementar uma Rede Neural Multicamada (MLP) para o dataset MNIST. O MNIST é um dataset de dígitos escritos a mão, conforme Figura 1. \n",
    "\n",
    "A MLP deve ser implementada de forma a utilizar uma arquitetura simples e ainda, assim, obter uma performance elevada. Ao final, três modelos de rede devem ser implementados e comparados. Os detalhes de implementação devem ser justificados a cada questão e será dividido em 5 etapas:\n",
    "* Leitura, visualização e Préprocessamento do dataset\n",
    "* Conversão do array de predições Y\n",
    "* Definição da topologia da rede (camadas e neurônios)\n",
    "* Definir otimizador, função custo e modo do treinamento (batch, mini-batch, estocástico)\n",
    "* Treinamento e avaliação de resultados\n",
    "\n",
    "![alt text](imgs/mninst.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura, visualização e pré-processamento do dataset\n",
    "\n",
    "Ao utilizar Redes Neurais para imagens, cada entrada é um pixel. Dessa forma, após a leitura do dataset, precisa-se descobrir as dimensões da imagem, a quantidade de instâncias, quantas classes e entradas são necessárias para o problema.\n",
    "\n",
    "Feito isso, algumas técnicas de pré-processamento devem ser utilizadas para melhores resultados, dentre eles converter a imagem para tons de cinza e normalizar.\n",
    "\n",
    "Sabendo disso, retorne na célula abaixo essas informações e gere uma versão normalizada do dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Exemplos de treinamento: (60000, 28, 28)\n",
      " Labels de treinamento: (60000,)\n",
      " Exemplos de teste: (10000, 28, 28)\n",
      " Labels de teste: (10000,)\n",
      "\n",
      "60000 Amostras de treinameto\n",
      "10000 Amostras de teste\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### leitura do dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "### Visualizar instâncias\n",
    "plt.imshow(x_train[5], cmap=plt.get_cmap('gray'))\n",
    "plt.title(y_train[5])\n",
    "### Print - informações das instâncias\n",
    "print(\" Exemplos de treinamento: \" + str(x_train.shape) + \"\\n\", \n",
    "      \"Labels de treinamento: \" + str(y_train.shape) + \"\\n\", \n",
    "      \"Exemplos de teste: \" + str(x_test.shape) + \"\\n\", \n",
    "      \"Labels de teste: \" + str(y_test.shape) + \"\\n\")\n",
    "### Normalizar\n",
    "x_train = x_train.reshape(60000,784).astype('float32') / 255\n",
    "x_test =  x_test.reshape(10000,784).astype('float32') / 255\n",
    "print(x_train.shape[0], 'Amostras de treinameto')\n",
    "print(x_test.shape[0], 'Amostras de teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversão do array de predições Y\n",
    "\n",
    "Baseado no número de classes do problema, represente a saída como one-hot encoding. Indique a importância dessa representação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "def one_hot(true_labels,num_classes):\n",
    "    \"\"\"\n",
    "    Função que implementa o one-hot encoding\n",
    "    Entrada: true_labels - array original com os labels\n",
    "    Retorna: labels - conversão one-hot          \n",
    "    \"\"\"\n",
    "    labels = keras.utils.to_categorical(true_labels, num_classes)\n",
    "    return labels\n",
    "\n",
    "y_train_oh = one_hot(y_train, num_classes)\n",
    "y_test_oh = one_hot(y_test, num_classes)\n",
    "print(y_train_oh.shape)\n",
    "print(y_test_oh.shape)\n",
    "print(y_test_oh[1])\n",
    "#print(x_train[5])\n",
    "#print(y_train_oh[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição de Topologia da Rede\n",
    "\n",
    "Mostre as três arquiteturas que serão utilizadas para resolver o problema. As arquiteturas devem ser simples de forma a retornar bons resultados com a menor quantidade necessária de neurônios e camadas. As três arquiteturas devem variar apenas quanto ao número de neurônios e camadas (incluindo regularização). Baseado nisso, explique:\n",
    "* Quantos neurônios serão utilizados em cada camada?\n",
    "* Quantas camadas serão utilizadas?\n",
    "* É necessário utilizar regularização? Se sim explique a técnica que foi utilizada e o impacto nos resultados\n",
    "* Qual função de ativação será utilizada? Escolha duas funções de ativação e explique os resultados obtidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_59 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.7242 - acc: 0.8016 - val_loss: 0.3010 - val_acc: 0.9165\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2736 - acc: 0.9228 - val_loss: 0.2338 - val_acc: 0.9337\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.2261 - acc: 0.9363 - val_loss: 0.2041 - val_acc: 0.9411\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1983 - acc: 0.9434 - val_loss: 0.1872 - val_acc: 0.9474\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1769 - acc: 0.9497 - val_loss: 0.1815 - val_acc: 0.9460\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1620 - acc: 0.9536 - val_loss: 0.1628 - val_acc: 0.9529\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.1489 - acc: 0.9554 - val_loss: 0.1575 - val_acc: 0.9542\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1392 - acc: 0.9585 - val_loss: 0.1453 - val_acc: 0.9564\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1302 - acc: 0.9612 - val_loss: 0.1420 - val_acc: 0.9574\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1215 - acc: 0.9638 - val_loss: 0.1389 - val_acc: 0.9572\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1152 - acc: 0.9661 - val_loss: 0.1319 - val_acc: 0.9603\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1077 - acc: 0.9684 - val_loss: 0.1345 - val_acc: 0.9607\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1026 - acc: 0.9698 - val_loss: 0.1333 - val_acc: 0.9600\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0989 - acc: 0.9709 - val_loss: 0.1274 - val_acc: 0.9607\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0944 - acc: 0.9718 - val_loss: 0.1269 - val_acc: 0.9597\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0907 - acc: 0.9730 - val_loss: 0.1229 - val_acc: 0.9625\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0863 - acc: 0.9744 - val_loss: 0.1202 - val_acc: 0.9623\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0825 - acc: 0.9757 - val_loss: 0.1205 - val_acc: 0.9622\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0795 - acc: 0.9761 - val_loss: 0.1250 - val_acc: 0.9618\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0763 - acc: 0.9772 - val_loss: 0.1264 - val_acc: 0.9611\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0733 - acc: 0.9781 - val_loss: 0.1173 - val_acc: 0.9632\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0699 - acc: 0.9794 - val_loss: 0.1218 - val_acc: 0.9640\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0679 - acc: 0.9796 - val_loss: 0.1175 - val_acc: 0.9632\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0648 - acc: 0.9804 - val_loss: 0.1163 - val_acc: 0.9644\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0627 - acc: 0.9812 - val_loss: 0.1220 - val_acc: 0.9630\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0622 - acc: 0.9816 - val_loss: 0.1195 - val_acc: 0.9647\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0583 - acc: 0.9828 - val_loss: 0.1200 - val_acc: 0.9646\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.0575 - acc: 0.9828 - val_loss: 0.1186 - val_acc: 0.9672\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0545 - acc: 0.9842 - val_loss: 0.1178 - val_acc: 0.9660\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0548 - acc: 0.9831 - val_loss: 0.1226 - val_acc: 0.9637\n",
      "Test loss: 0.12257182073467411\n",
      "Test accuracy: 0.9637\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='sigmoid'))\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train_oh, batch_size=256, epochs=30, verbose=1, validation_data=(x_test, y_test_oh))\n",
    "\n",
    "score = model.evaluate(x_test, y_test_oh, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_70 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 29,642\n",
      "Trainable params: 29,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.7060 - acc: 0.7991 - val_loss: 0.2732 - val_acc: 0.9248\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.2426 - acc: 0.9300 - val_loss: 0.2152 - val_acc: 0.9364\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1965 - acc: 0.9430 - val_loss: 0.2093 - val_acc: 0.9381\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1686 - acc: 0.9507 - val_loss: 0.1741 - val_acc: 0.9477\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1476 - acc: 0.9577 - val_loss: 0.1509 - val_acc: 0.9571\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1345 - acc: 0.9610 - val_loss: 0.1543 - val_acc: 0.9563\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1229 - acc: 0.9639 - val_loss: 0.1463 - val_acc: 0.9569\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1146 - acc: 0.9665 - val_loss: 0.1345 - val_acc: 0.9609\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1058 - acc: 0.9682 - val_loss: 0.1230 - val_acc: 0.9632\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0989 - acc: 0.9705 - val_loss: 0.1238 - val_acc: 0.9633\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0925 - acc: 0.9721 - val_loss: 0.1146 - val_acc: 0.9633\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0853 - acc: 0.9746 - val_loss: 0.1202 - val_acc: 0.9640\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0809 - acc: 0.9754 - val_loss: 0.1163 - val_acc: 0.9641\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0758 - acc: 0.9766 - val_loss: 0.1244 - val_acc: 0.9642\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0724 - acc: 0.9777 - val_loss: 0.1090 - val_acc: 0.9683\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0690 - acc: 0.9787 - val_loss: 0.1133 - val_acc: 0.9667\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0652 - acc: 0.9798 - val_loss: 0.1103 - val_acc: 0.9688\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0615 - acc: 0.9812 - val_loss: 0.1077 - val_acc: 0.9698\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0582 - acc: 0.9822 - val_loss: 0.1100 - val_acc: 0.9670\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0567 - acc: 0.9825 - val_loss: 0.1073 - val_acc: 0.9686\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0524 - acc: 0.9838 - val_loss: 0.1169 - val_acc: 0.9683\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0502 - acc: 0.9842 - val_loss: 0.1110 - val_acc: 0.9669\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0495 - acc: 0.9841 - val_loss: 0.1098 - val_acc: 0.9703\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0463 - acc: 0.9855 - val_loss: 0.1116 - val_acc: 0.9686\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0436 - acc: 0.9866 - val_loss: 0.1069 - val_acc: 0.9714\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0409 - acc: 0.9876 - val_loss: 0.1158 - val_acc: 0.9687\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0381 - acc: 0.9882 - val_loss: 0.1130 - val_acc: 0.9691\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0363 - acc: 0.9887 - val_loss: 0.1145 - val_acc: 0.9694\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0363 - acc: 0.9882 - val_loss: 0.1228 - val_acc: 0.9669\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0324 - acc: 0.9897 - val_loss: 0.1125 - val_acc: 0.9709\n",
      "Test loss: 0.11252735372448223\n",
      "Test accuracy: 0.9709\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='sigmoid'))\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train_oh, batch_size=256, epochs=30, verbose=1, validation_data=(x_test, y_test_oh))\n",
    "\n",
    "score = model.evaluate(x_test, y_test_oh, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_82 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 53,018\n",
      "Trainable params: 53,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.7532 - acc: 0.7760 - val_loss: 0.2943 - val_acc: 0.9169\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2549 - acc: 0.9275 - val_loss: 0.2207 - val_acc: 0.9340\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1969 - acc: 0.9432 - val_loss: 0.1855 - val_acc: 0.9413\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1637 - acc: 0.9524 - val_loss: 0.1630 - val_acc: 0.9493\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1409 - acc: 0.9590 - val_loss: 0.1482 - val_acc: 0.9546\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1243 - acc: 0.9636 - val_loss: 0.1338 - val_acc: 0.9591\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1118 - acc: 0.9671 - val_loss: 0.1274 - val_acc: 0.9604\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.1007 - acc: 0.9702 - val_loss: 0.1231 - val_acc: 0.9619\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0910 - acc: 0.9730 - val_loss: 0.1140 - val_acc: 0.9655\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0835 - acc: 0.9752 - val_loss: 0.1095 - val_acc: 0.9668\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0772 - acc: 0.9773 - val_loss: 0.1141 - val_acc: 0.9665\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0717 - acc: 0.9789 - val_loss: 0.1029 - val_acc: 0.9687\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0641 - acc: 0.9809 - val_loss: 0.1130 - val_acc: 0.9662\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0600 - acc: 0.9818 - val_loss: 0.1010 - val_acc: 0.9719\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0556 - acc: 0.9840 - val_loss: 0.0997 - val_acc: 0.9707\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0532 - acc: 0.9837 - val_loss: 0.1081 - val_acc: 0.9683\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0485 - acc: 0.9854 - val_loss: 0.1028 - val_acc: 0.9703\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.0471 - acc: 0.9858 - val_loss: 0.1049 - val_acc: 0.9699\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0415 - acc: 0.9875 - val_loss: 0.1072 - val_acc: 0.9700\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0381 - acc: 0.9887 - val_loss: 0.0994 - val_acc: 0.9731\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0373 - acc: 0.9890 - val_loss: 0.1059 - val_acc: 0.9704\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0342 - acc: 0.9895 - val_loss: 0.1081 - val_acc: 0.9715\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0301 - acc: 0.9913 - val_loss: 0.1102 - val_acc: 0.9699\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0278 - acc: 0.9919 - val_loss: 0.1046 - val_acc: 0.9726\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0258 - acc: 0.9923 - val_loss: 0.1069 - val_acc: 0.9726\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0251 - acc: 0.9923 - val_loss: 0.1027 - val_acc: 0.9741\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0238 - acc: 0.9927 - val_loss: 0.1144 - val_acc: 0.9714\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0217 - acc: 0.9935 - val_loss: 0.1054 - val_acc: 0.9744\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0197 - acc: 0.9944 - val_loss: 0.1273 - val_acc: 0.9680\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0197 - acc: 0.9944 - val_loss: 0.1073 - val_acc: 0.9736\n",
      "Test loss: 0.10726654627741664\n",
      "Test accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='sigmoid'))\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train_oh, batch_size=256, epochs=30, verbose=1, validation_data=(x_test, y_test_oh))\n",
    "\n",
    "score = model.evaluate(x_test, y_test_oh, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir otimizador, função custo e modo do treinamento\n",
    "\n",
    "Uma Rede Neural é um problema de otimização, sabendo disso, explique:\n",
    "* Qual otimizador será utilizado? Escolha dois otimizadores, explique a diferença entre eles justificando a escolha e comente os resultados obtidos.\n",
    "* Será necessário utilizar uma função custo? Explique o que é uma função custo e qual será utilizada justificando a escolha.\n",
    "* Qual modo de treinamento foi utilizado? Justifique sua resposta\n",
    "* Quantas épocas serão necessárias? Caso os métodos por batch ou mini-batch sejam escolhidos, indique também o tamanho\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento e avaliação dos resultados\n",
    "\n",
    "Treine as três redes e mostre os resultados de cada uma a partir das questões anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
